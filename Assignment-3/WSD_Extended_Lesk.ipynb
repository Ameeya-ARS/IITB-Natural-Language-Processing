{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTyjjcpBzXNC",
        "outputId": "889b5e35-be90-4cbf-ba2e-24e4cfc542d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model_w2v = KeyedVectors.load_word2vec_format('drive/MyDrive/CS626/Assignment_2/GoogleNews-vectors-negative300.bin',binary=True)"
      ],
      "metadata": {
        "id": "UDzaytAr0HF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import semcor\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('semcor')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import re\n",
        "import warnings\n",
        "from sklearn import metrics\n",
        "from nltk import word_tokenize\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WnN42aW0bAt",
        "outputId": "09ba6e1e-fd49-47ce-c3d4-54098dd6da19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcor_tagged_sents = semcor.tagged_sents(tag='sem')"
      ],
      "metadata": {
        "id": "K8ld3dBHVO7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semcor_tagged_sents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQLrPx0JVl_E",
        "outputId": "d421bf58-47f3-4edc-9a95-af6167b83f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The'],\n",
              " Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]),\n",
              " Tree(Lemma('state.v.01.say'), ['said']),\n",
              " Tree(Lemma('friday.n.01.Friday'), ['Friday']),\n",
              " ['an'],\n",
              " Tree(Lemma('probe.n.01.investigation'), ['investigation']),\n",
              " ['of'],\n",
              " Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']),\n",
              " [\"'s\"],\n",
              " Tree(Lemma('late.s.03.recent'), ['recent']),\n",
              " Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']),\n",
              " Tree(Lemma('produce.v.04.produce'), ['produced']),\n",
              " ['``'],\n",
              " ['no'],\n",
              " Tree(Lemma('evidence.n.01.evidence'), ['evidence']),\n",
              " [\"''\"],\n",
              " ['that'],\n",
              " ['any'],\n",
              " Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']),\n",
              " Tree(Lemma('happen.v.01.take_place'), ['took', 'place']),\n",
              " ['.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = {}\n",
        "semcor_sents = semcor.sents()\n",
        "for sentence in semcor_sents:\n",
        "    for word in sentence:\n",
        "        if word in model_w2v:\n",
        "            vectors[word] = model_w2v.get_vector(word)"
      ],
      "metadata": {
        "id": "CaHT9D6u0fde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_vec(sentence):\n",
        "    li = []\n",
        "    for word in sentence:\n",
        "        if word in vectors:\n",
        "            li.append(vectors[word])\n",
        "    if len(li) == 0:\n",
        "        return np.zeros(300)\n",
        "    return np.average(li,axis=0)"
      ],
      "metadata": {
        "id": "UM5JLno20gWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sense_dict(sents):\n",
        "    sense_set = set()\n",
        "    for i in range(len(sents)):\n",
        "        for j in range(len(sents[i])):\n",
        "            if isinstance(sents[i][j], nltk.Tree):\n",
        "                try :\n",
        "                    if sents[i][j].height() == 3:\n",
        "                        for tree in sents[i][j]:\n",
        "                            if(tree.label() == 'NE'):\n",
        "                                sense = 'NE'\n",
        "                                sense_set.add(sense) \n",
        "                    else :\n",
        "                        sense = sents[i][j].label().synset().name()\n",
        "                        sense_set.add(sense)\n",
        " \n",
        "                except:\n",
        "                    if(sents[i][j].label() == 'NE'):\n",
        "                        sense = sents[i][j].label()\n",
        "                        sense_set.add(sense)\n",
        "                    else :\n",
        "                        sense = sents[i][j].label()\n",
        "                        sense_set.add(sense)\n",
        "            else :\n",
        "                sense = 'notag'\n",
        "                sense_set.add(sense)\n",
        "    sense_set.add('unk')\n",
        "    sense_set = list(sense_set)\n",
        "    sense_dict = {sense : i for i, sense in enumerate(sense_set)}\n",
        "    return sense_dict"
      ],
      "metadata": {
        "id": "xP5UqGBjdX4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def predict_synset_extended(word, sentence):\n",
        "    sentence = [w for w in sentence if (w not in stop_words) and (w.isalnum())]\n",
        "\n",
        "    if word in sentence:\n",
        "        sentence.remove(word)\n",
        "        \n",
        "    context_bag = sent_vec(sentence)\n",
        "    \n",
        "    \n",
        "    sense_bag = {}\n",
        "    senses = wn.synsets(word)\n",
        "\n",
        "    if len(senses)>0:\n",
        "        for sense in senses:\n",
        "            sense_bag[sense] = []\n",
        "            sense_bag[sense].append(sent_vec([w for w in sense.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
        "\n",
        "            for hypo in sense.hyponyms():\n",
        "                sense_bag[sense].append(sent_vec([w for w in hypo.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
        "        \n",
        "        if len(sense_bag.keys()) > 0:\n",
        "            synset = \"\"\n",
        "\n",
        "            cos_sims = []\n",
        "            for key,val in sense_bag.items():\n",
        "                cos_sims.append((key,model_w2v.cosine_similarities(context_bag,val).mean()))#,model_w2v.cosine_similarities(context_bag,val)))\n",
        "            cos_sims.sort(key = lambda x: x[1],reverse=True)\n",
        "            return str(cos_sims[0][0])[8:-2]\n",
        "    else:\n",
        "        return \"notag\"\n"
      ],
      "metadata": {
        "id": "1m7JQ4DG0iz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sents, untagged_sents):\n",
        "    pred_sense = []\n",
        "    actual_sense = []\n",
        "    for i in range(len(sents)):\n",
        "        for j in range(len(sents[i])):\n",
        "            if isinstance(sents[i][j], nltk.Tree):\n",
        "                try :\n",
        "                    if sents[i][j].height() == 3:\n",
        "                        for tree in sents[i][j]:\n",
        "                            if(tree.label() == 'NE'):\n",
        "                                sense = 'NE'\n",
        "                                actual_sense.append(sense)\n",
        "                                word = \"_\".join(tree.leaves())\n",
        "                    else :\n",
        "                        sense = sents[i][j].label().synset().name()\n",
        "                        actual_sense.append(sense)\n",
        "                        word = \"_\".join(sents[i][j].leaves())\n",
        "                    pred_sense.append(predict_synset_extended(word, untagged_sents[i]))                   \n",
        "                except:\n",
        "                    if(sents[i][j].label() == 'NE'):\n",
        "                        sense = sents[i][j].label()\n",
        "                        word = \"_\".join(sents[i][j].leaves())\n",
        "                        actual_sense.append(sense)\n",
        "                    else :\n",
        "                        sense = sents[i][j].label()\n",
        "                        word = \"_\".join(sents[i][j].leaves())\n",
        "                        actual_sense.append(sense)\n",
        "                    pred_sense.append(predict_synset_extended(word, untagged_sents[i]))                \n",
        "            else :\n",
        "                sense = 'notag'\n",
        "                actual_sense.append(sense)\n",
        "                word = \"_\".join(sents[i][j])\n",
        "                pred_sense.append(predict_synset_extended(word, untagged_sents[i]))           \n",
        "    return actual_sense, pred_sense"
      ],
      "metadata": {
        "id": "GqH7xAZ8dl5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = semcor.tagged_sents(tag='sem')\n",
        "sense_dict = get_sense_dict(sents)\n",
        "untagged_sents = semcor.sents()\n",
        "y_true, y_pred = predict(sents, untagged_sents)\n",
        "\n",
        "y_true = [sense_dict[sense] for sense in y_true]\n",
        "y_pred = [sense_dict.get(sense, sense_dict['notag']) for sense in y_pred]\n"
      ],
      "metadata": {
        "id": "uRg9yJ4-eBDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_map = {v: k for k, v in sense_dict.items()}\n",
        "y_true = [inv_map[i] for i in y_true]\n",
        "y_pred = [inv_map[i] for i in y_pred]\n",
        "y_true_n, y_pred_n = [],[]\n",
        "for i in range(len(y_true)):\n",
        "  if(y_true[i] != 'notag' and y_true[i] != 'NE'):\n",
        "    temp = y_true[i].split('.')[1]\n",
        "    if(temp=='n'):\n",
        "      y_true_n.append(y_true[i])\n",
        "      y_pred_n.append(y_pred[i])"
      ],
      "metadata": {
        "id": "oeCtScWRI6PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy : \", metrics.accuracy_score(y_true_n, y_pred_n))\n",
        "precision, recall, f1score, _ = metrics.precision_recall_fscore_support(y_true, y_pred, average='weighted',zero_division=0)\n",
        "print(\"Precision : \", precision)\n",
        "print(\"Recall : \", recall)\n",
        "print(\"F1 Score : \", f1score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WpKLyvDkXU5",
        "outputId": "8d9e55c6-81a0-4a4b-f8df-d589cef1d10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  0.4207885214277609\n",
            "Precision :  0.7172896749077547\n",
            "Recall :  0.5928444733857615\n",
            "F1 Score :  0.6267663941848407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"She likes to play cricket\"\n",
        "tokens = word_tokenize(test_sentence)\n",
        "for word in tokens:\n",
        "  pred_sense = predict_synset_extended(word, tokens)\n",
        "  if(pred_sense!='notag'):\n",
        "    print(\"Target Word : \" + word + \"\\nPredicted Sense : \" + pred_sense + \" : \" + wn.synset(pred_sense).definition() + \"\\n\")\n",
        "  else:\n",
        "    print(\"Target Word : \" + word + \"\\nPredicted Sense : \" + pred_sense + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlKn-QQ-T043",
        "outputId": "88a36c50-b758-4a78-abbe-431c7366eb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Word : She\n",
            "Predicted Sense : notag\n",
            "\n",
            "Target Word : likes\n",
            "Predicted Sense : like.n.02 : a kind of person\n",
            "\n",
            "Target Word : to\n",
            "Predicted Sense : notag\n",
            "\n",
            "Target Word : play\n",
            "Predicted Sense : playing_period.n.01 : (in games or plays or other performances) the time during which play proceeds\n",
            "\n",
            "Target Word : cricket\n",
            "Predicted Sense : cricket.n.02 : a game played with a ball and bat by two teams of 11 players; teams take turns trying to score runs\n",
            "\n"
          ]
        }
      ]
    }
  ]
}