{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wrU9xiiN6WH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.util import ngrams\n",
        "import pickle\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxCy1dNegwow",
        "outputId": "e41dba93-5718-47e1-8987-c9bcb7354374"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eXV6Q_gOLd4",
        "outputId": "fec41ca0-e0d3-44a1-9753-f981703599e7"
      },
      "outputs": [],
      "source": [
        "!pip install skipthoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npMdYRKVOVhg",
        "outputId": "6d63d623-4569-41c0-9f78-e5b9488ee285"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkfaT77ROesD"
      },
      "outputs": [],
      "source": [
        "my_dict = pickle.load(open('gdrive/MyDrive/CS626/Project/Data/my_dict.pkl', 'rb'))\n",
        "biskip = pickle.load(open('gdrive/MyDrive/CS626/Project/Data/biskip.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmOhwXvVOv7u"
      },
      "outputs": [],
      "source": [
        "def neural_features(headlines,bodies):\n",
        "  MAX_HEAD = max(len(ele) for ele in headlines)\n",
        "  MAX_BODY = max(len(ele) for ele in bodies)\n",
        "  headlines_to_ids = np.zeros((len(headlines),MAX_HEAD+1))\n",
        "  bodies_to_ids = np.zeros((len(bodies),MAX_BODY+1))\n",
        "  headlines_encodings = np.zeros((len(headlines),2400))\n",
        "  bodies_encodings = np.zeros((len(bodies),2400))\n",
        "  for i in range(len(headlines)):\n",
        "    headline = headlines[i].split()\n",
        "    headline.append('<eos>')\n",
        "    body = bodies[i].split()\n",
        "    body.append('<eos>')\n",
        "    j=0\n",
        "    for word in headline:\n",
        "      try:\n",
        "        headlines_to_ids[i][j] = my_dict[word]\n",
        "      except KeyError:\n",
        "        pass\n",
        "      j+=1\n",
        "    j=0\n",
        "    for word in body:\n",
        "      try:\n",
        "        bodies_to_ids[i][j] = my_dict[word]\n",
        "      except KeyError:\n",
        "        pass\n",
        "      j+=1\n",
        "  last_temp = len(headlines) - len(headlines)%50\n",
        "  for i in range(0,len(headlines),50):\n",
        "    # print(i)\n",
        "    input1 = Variable(torch.LongTensor(headlines_to_ids[i:i+50]))\n",
        "    input2 = Variable(torch.LongTensor(bodies_to_ids[i:i+50]))\n",
        "    headline_output = biskip(input1).detach().numpy()\n",
        "    body_output = biskip(input2).detach().numpy()\n",
        "    headlines_encodings[i:i+50] = headline_output[0:50]\n",
        "    bodies_encodings[i:i+50] = body_output[0:50]\n",
        "  if(last_temp != len(headlines)):\n",
        "    input1 = Variable(torch.LongTensor(headlines_to_ids[last_temp:]))\n",
        "    input2 = Variable(torch.LongTensor(bodies_to_ids[last_temp:]))\n",
        "    headline_output = biskip(input1).detach().numpy()\n",
        "    body_output = biskip(input2).detach().numpy()\n",
        "    headlines_encodings[last_temp:] = headline_output[:]\n",
        "    bodies_encodings[last_temp:] = body_output[:]\n",
        "\n",
        "  feat1 = np.zeros((len(headlines),2400))\n",
        "  feat2 = np.zeros((len(headlines),2400))\n",
        "  i = 0\n",
        "  for h_vector,b_vector in zip(headlines_encodings,bodies_encodings):\n",
        "    feat1[i] = np.multiply(h_vector,b_vector)\n",
        "    feat2[i] = np.absolute(h_vector-b_vector)\n",
        "    i+=1\n",
        "\n",
        "  final_neural_features = np.concatenate((feat1,feat2),axis = 1)\n",
        "  return final_neural_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM1zH0r3QPUZ"
      },
      "outputs": [],
      "source": [
        "head_demo_sent1 = 'ISIL Beheads American Photojournalist in Iraq' #discuss\n",
        "body_demo_sent1 = 'James Foley, an American journalist who went missing in Syria more than a year ago, has reportedly been executed by the Islamic State, a militant group formerly known as ISIS.Video and photos purportedly of Foley emerged on Tuesday. A YouTube video -- entitled \"A Message to #America (from the #IslamicState)\" -- identified a man on his knees as \"James Wright Foley,\" and showed his execution.This is a developing story. Check back here for updates.'\n",
        "head_demo_sent2 = 'A Russian Guy Says His Justin Bieber Ringtone Saved Him From A Bear Attack' #unrelated\n",
        "body_demo_sent2 = 'A bereaved Afghan mother took revenge on the Taliban after watching them kill her son in an ambush. Reza Gul killed 25 Taliban fighters and injured five others in a seven-hour gunbattle in Farah province.Gul, who was joined by her daughter and daughter in-law, engaged the Taliban using AK-47s and grenades, despitenever before having used a weapon.The embattled mother told Tolo news, a 24-hour Afghan news broadcaster, she was awakened by shots early Tuesday. After seeing that her son had been killed, Gul and the other two women fought back.“I couldn stop myself and picked up a weapon,” Gul told Tolo News. “I went to the check post and began shooting back.”Seema, her daughter-in-law, added: “The fighting was intensified when we reached the battlefield along with light and heavy weapons. We were committed to fight until the last bullet.”Gul said that the battlefield was covered in Talib fighters after the deadly exchange ended.While the Taliban have not publicly commented on the incident, the Afghan government labeled it a symbol of a public uprising against the Taliban.Taliban and other groups have regained large swathes of the country as U.S. and NATO forces slowly pull out troops after 14 years of war. The Taliban have targeted government and foreign infrastructure as the group attempts to claw back power it lost in 2001.While the Taliban have made key gains in rural regions, members continue to employ suicide bomber tactics in well protected towns and cities. Earlier this week, 50 people were killed after a suicide bomber detonated a vest during a volleyball competition in Yahyakahil, Paktika province.That particular attack prompted President Ashraf Ghani to order a complete overview of the country’s defense forces and to rethink the ban on nighttime raids, which were outlawed by his predecessor, Hamid Karzai.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B5Vck1fQJGC"
      },
      "outputs": [],
      "source": [
        "headlines_org = []\n",
        "bodies_org = []\n",
        "headlines_org.append(head_demo_sent1)\n",
        "bodies_org.append(body_demo_sent1)\n",
        "headlines_org.append(head_demo_sent2)\n",
        "bodies_org.append(body_demo_sent2)\n",
        "n_feats = neural_features(headlines_org,bodies_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av3XyWl0RSTN",
        "outputId": "911d0a25-5cf3-477b-e60c-be2a30608d3f"
      },
      "outputs": [],
      "source": [
        "print(n_feats.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QckKG9FPR5eE",
        "outputId": "bc45bc23-0853-4b90-edc0-0b0bd33bd9a1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efKc9kKbSAeN"
      },
      "outputs": [],
      "source": [
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    # Removes stopwords from a list of tokens\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "def preprocess(headlines,bodies):\n",
        "  n_headlines, n_bodies =[],[]\n",
        "  for i, (headline, body) in enumerate(zip(headlines, bodies)):\n",
        "    clean_headline = clean(headline)\n",
        "    clean_body = clean(body)\n",
        "    clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "    clean_body = get_tokenized_lemmas(clean_body)\n",
        "    clean_headline = remove_stopwords(clean_headline)\n",
        "    clean_body = remove_stopwords(clean_body)\n",
        "    n_headlines.append(headline)\n",
        "    n_bodies.append(body)\n",
        "  n_headlines_df=pd.DataFrame(n_headlines,columns=['Headline'])\n",
        "  n_bodies_df=pd.DataFrame(n_bodies,columns=['Body'])\n",
        "  return n_headlines_df['Headline'], n_bodies_df['Body']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNxEjVkgSG_A"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('gdrive/MyDrive/CS626/Project/Data/train_Set.csv')\n",
        "def statistical_features(headlines,bodies):\n",
        "  stop_words_l=stopwords.words('english')\n",
        "  org_hs, org_bs = df['Headline'], df['Body']\n",
        "  org_hs,org_bs = preprocess(org_hs,org_bs)\n",
        "  headline_vectorizer = TfidfVectorizer()\n",
        "  h1 = headline_vectorizer.fit(org_hs)\n",
        "  h = h1.transform(headlines)\n",
        "  body_vectorizer = TfidfVectorizer(max_features=10000-h.shape[1])\n",
        "  b1 = body_vectorizer.fit(org_bs)\n",
        "  b = b1.transform(bodies)\n",
        "  statistical_features = np.concatenate((np.array(h.toarray()),np.array(b.toarray())),axis = 1)\n",
        "  return statistical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBN9PLNuSlQ2"
      },
      "outputs": [],
      "source": [
        "s_feats = statistical_features(headlines_org,bodies_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1byTD0KuYR_b",
        "outputId": "456783e3-45a9-4184-cb61-9716b08deed7"
      },
      "outputs": [],
      "source": [
        "print(s_feats.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZB-4DQIYb3-",
        "outputId": "95854528-4e14-42ae-8fa3-c7db32fe87ec"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Uuy_CVYcge"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz-B3jRZYgBe"
      },
      "outputs": [],
      "source": [
        "def external_features(headline,body):  \n",
        "  eng_ext = []\n",
        "  i = 0\n",
        "  for sent1,sent2 in zip(headline,body):\n",
        "    i+=1\n",
        "    vec = []\n",
        "\n",
        "    #character ngrams\n",
        "    for n in range(2,17):\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(set(n_grams_2)))))\n",
        "      temp_c1=0\n",
        "      temp_c2=0\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_3 = ngrams(sent2.lower()[:255],n)\n",
        "      temp_c1 = len(list(set(n_grams_1).intersection(set(n_grams_3))))\n",
        "      n_grams_1 = ngrams(sent1.lower(), n)\n",
        "      n_grams_4 = ngrams(sent2.lower()[:100],n)\n",
        "      temp_c2 = len(list(set(n_grams_1).intersection(set(n_grams_4))))\n",
        "      vec.append(temp_c1)\n",
        "      vec.append(temp_c2)\n",
        "\n",
        "    #word ngrams\n",
        "    for n in range(2,7):\n",
        "      n_grams_1 = ngrams(sent1.lower().split(), n)\n",
        "      n_grams_2 = ngrams(sent2.lower().split(),n)\n",
        "      vec.append(len(list(set(n_grams_1).intersection(set(n_grams_2)))))\n",
        "      temp_c=0\n",
        "      n_grams_1 = ngrams(sent1.lower().split(), n)\n",
        "      n_grams_3 = ngrams(sent2.lower()[:255].split(),n)\n",
        "      temp_c=len(list(set(n_grams_1).intersection(set(n_grams_3))))\n",
        "      vec.append(temp_c)\n",
        "\n",
        "    #no of common words between headline and body with respect to total words\n",
        "    s1 = sent1.split()\n",
        "    s2 = sent2.split()\n",
        "    vec.append(len(set(s1).intersection(s2)) / float(len(set(s1).union(s2))))\n",
        "\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    d1 = sid_obj.polarity_scores(sent1)\n",
        "    d2 = sid_obj.polarity_scores(sent2)\n",
        "    vec.append(np.absolute(d1['neg']-d2['neg']))\n",
        "    vec.append(np.absolute(d1['neu']-d2['neu']))\n",
        "    vec.append(np.absolute(d1['pos']-d2['pos']))\n",
        "    vec.append(np.absolute(d1['compound']-d2['compound']))\n",
        "\n",
        "    eng_ext.append(vec)\n",
        "\n",
        "  eng_ext = np.array(eng_ext)\n",
        "  return eng_ext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCwnjm4AYuEc"
      },
      "outputs": [],
      "source": [
        "e_feats = external_features(headlines_org,bodies_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k262VShGZb02",
        "outputId": "eb1fbc5e-c00b-4154-9f0d-eec03fac1fc2"
      },
      "outputs": [],
      "source": [
        "print(e_feats.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNsuZXmhZ8V1"
      },
      "outputs": [],
      "source": [
        "model = pickle.load(open('gdrive/MyDrive/CS626/Project/Data/final_model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mGCtiFfaFVy",
        "outputId": "bf1fca28-ec9a-4774-a533-28e4eaad10ac"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x=[n_feats,e_feats,s_feats])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XZutxZaaOkn"
      },
      "outputs": [],
      "source": [
        "y_pred_ = np.argmax(y_pred,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_br1myxollP",
        "outputId": "aff46277-d997-4862-b457-38223cb74b7e"
      },
      "outputs": [],
      "source": [
        "print(y_pred_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm94KoitaQ6x",
        "outputId": "4e04117f-10e6-4b60-ae47-1dcc0afa2da9"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "  if(y_pred_[i] == 0):\n",
        "      print(\"Headline and body pairs agree with each other\")\n",
        "  elif(y_pred_[i] == 1):\n",
        "      print(\"Headline and body pairs disagree with each other\")\n",
        "  elif(y_pred_[i]==2):\n",
        "      print(\"Headline and body pairs are discussion statements\")\n",
        "  elif(y_pred_[i]==3):\n",
        "      print(\"Headline and body pairs are unrelated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN7BKLsPgmHL"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "class_names=['agree','disagree','discussion','unrelated']\n",
        "explainer= LimeTextExplainer(class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fLGKsLXgo3_"
      },
      "outputs": [],
      "source": [
        "def predict_proba(arr):\n",
        "  processed = []\n",
        "  final_res = np.zeros((len(arr),4))\n",
        "  i_ =0\n",
        "  final_headlines = []\n",
        "  final_bodies = []\n",
        "  for i in arr:\n",
        "    a = i.split('\\n',1)\n",
        "    headline = a[0]\n",
        "    body = a[1]\n",
        "    final_headlines.append(headline)\n",
        "    final_bodies.append(body)\n",
        "  res = model.predict(x=[neural_features(final_headlines,final_bodies),external_features(final_headlines,final_bodies),statistical_features(final_headlines,final_bodies)])\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15FcpWZZgq6M",
        "outputId": "ac2fb17e-5bbe-4df8-f940-b293cb8b031f"
      },
      "outputs": [],
      "source": [
        "exp = explainer.explain_instance(head_demo_sent1+'\\n'+body_demo_sent1,predict_proba,num_features=20,top_labels=4,num_samples=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "OlWLoyDZgt5u",
        "outputId": "163b74d5-f3ca-4f9e-a22a-679ed14424a5"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(text=True,labels=(2,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mekHbtEDGBnJ",
        "outputId": "b5396570-b17f-4e2b-cbd1-1e3dcc2b5b4f"
      },
      "outputs": [],
      "source": [
        "exp2 = explainer.explain_instance(head_demo_sent2+'\\n'+body_demo_sent2,predict_proba,num_features=20,top_labels=4,num_samples=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "hJlb5wkTGF4B",
        "outputId": "acff1141-f468-447a-9c03-8799315187c6"
      },
      "outputs": [],
      "source": [
        "exp2.show_in_notebook(text=True,labels=(3,))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
