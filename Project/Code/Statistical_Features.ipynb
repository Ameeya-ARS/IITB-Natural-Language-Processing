{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdPk4RXX0sXQ",
        "outputId": "b6e65a69-12ad-45fa-a0ea-4a90d131df9b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wElsBCuU1Fsl",
        "outputId": "43d803f6-ad06-4b91-c990-d95f43118a0a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtSitsIJn_mG"
      },
      "outputs": [],
      "source": [
        "\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    # Removes stopwords from a list of tokens\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "def preprocess(headlines,bodies):\n",
        "  n_headlines, n_bodies =[],[]\n",
        "  for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "    clean_headline = clean(headline)\n",
        "    clean_body = clean(body)\n",
        "    clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "    clean_body = get_tokenized_lemmas(clean_body)\n",
        "    clean_headline = remove_stopwords(clean_headline)\n",
        "    clean_body = remove_stopwords(clean_body)\n",
        "    n_headlines.append(headline)\n",
        "    n_bodies.append(body)\n",
        "  n_headlines_df=pd.DataFrame(n_headlines,columns=['Headline'])\n",
        "  n_bodies_df=pd.DataFrame(n_bodies,columns=['Body'])\n",
        "  return n_headlines_df['Headline'], n_bodies_df['Body']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IDI5ocz1Llx"
      },
      "outputs": [],
      "source": [
        "def statistical_features(dataset_loc):\n",
        "  df = pd.read_csv('gdrive/MyDrive/CS626/Project/Data/train_Set.csv')\n",
        "  dataset = pd.read_csv(dataset_loc)\n",
        "  stop_words_l=stopwords.words('english')\n",
        "  headlines = dataset['Headline']\n",
        "  bodies = dataset['Body']\n",
        "  headlines,bodies = preprocess(headlines,bodies)\n",
        "  df['Headline'], df['Body'] = preprocess(df['Headline'],df['Body'])\n",
        "  headline_vectorizer = TfidfVectorizer()\n",
        "  h1 = headline_vectorizer.fit(df['Headline'])\n",
        "  h = h1.transform(headlines)\n",
        "  body_vectorizer = TfidfVectorizer(max_features=10000-h.shape[1])\n",
        "  b1 = body_vectorizer.fit(df['Body'])\n",
        "  b = b1.transform(bodies)\n",
        "  statistical_features = np.concatenate((np.array(h.toarray()),np.array(b.toarray())),axis = 1)\n",
        "  return statistical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2bLM-9a2eSC",
        "outputId": "72f0f4c0-4712-429f-8621-ca81235d4dbb"
      },
      "outputs": [],
      "source": [
        "statistical_features_train = statistical_features('gdrive/MyDrive/CS626/Project/Data/train_Set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77NL6hh72x2j",
        "outputId": "dd355886-dc4b-4f72-8b7d-a63df5e2edba"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(statistical_features_train[500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlEXX9LS2108",
        "outputId": "411928a6-c098-4423-a0f5-150a614dbff9"
      },
      "outputs": [],
      "source": [
        "statistical_features_test = statistical_features('gdrive/MyDrive/CS626/Project/Data/test_Set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKeJHDwo26hc",
        "outputId": "69598c94-ed4f-4e20-d6a8-8855066ba9b0"
      },
      "outputs": [],
      "source": [
        "statistical_features_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJDcdtni2_D7"
      },
      "outputs": [],
      "source": [
        "np.save(arr=statistical_features_test,file='gdrive/MyDrive/CS626/Project/Data/test_statistical_features.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iOIXp1M8I7O"
      },
      "outputs": [],
      "source": [
        "np.save(arr=statistical_features_train,file='gdrive/MyDrive/CS626/Project/Data/train_statistical_features.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SyMPSQs31Sn",
        "outputId": "e1b70f02-7270-4e8e-90db-f1066c05fb93"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(statistical_features_test[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
